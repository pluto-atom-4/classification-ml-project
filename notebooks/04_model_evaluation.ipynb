{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Training and Comparison\n",
    "\n",
    "1. Data preparation with your custom preprocessing pipeline\n",
    "2. Model configuration with multiple algorithms\n",
    "3. Cross-validation for robust performance estimates\n",
    "4. Comprehensive comparison with multiple metrics\n",
    "5. Visualizations including bar charts, radar charts, confusion matrices, and ROC curves\n",
    "6. Feature importance analysis for interpretable models\n",
    "7. Final summary with recommendations"
   ],
   "id": "b4da13e5c0cf35ac"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_preprocessing import DataLoader, create_pipeline\n",
    "from models import ModelTrainer, BinaryClassifier, cross_validate_model\n",
    "from evaluation import ClassificationEvaluator, compare_models, print_evaluation_summary"
   ],
   "id": "51bc11462f3fd69b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading and Preprocessing",
   "id": "739297785ad58774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load and prepare the dataset\n",
    "loader = DataLoader()\n",
    "df = loader.load_sklearn_dataset(\"breast_cancer\", save_raw=True)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "X_train, X_test, y_train, y_test, preprocessor = create_pipeline(\n",
    "    df,\n",
    "    target_column=\"target\",\n",
    "    handle_missing=True,\n",
    "    encode_categorical=True,\n",
    "    scale_features=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Class distribution:\\n{pd.Series(y_train).value_counts()}\")"
   ],
   "id": "85e9ebd4cac48ca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Configuration and Training",
   "id": "2901d0c9c0123d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model configurations\n",
    "model_configs = {\n",
    "    \"Random Forest\": {\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_samples_split\": 5\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model_type\": \"logistic_regression\",\n",
    "        \"C\": 1.0,\n",
    "        \"max_iter\": 1000\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model_type\": \"svm\",\n",
    "        \"C\": 1.0,\n",
    "        \"kernel\": \"rbf\",\n",
    "        \"probability\": True\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model_type\": \"gradient_boosting\",\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 6\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model_type\": \"decision_tree\",\n",
    "        \"max_depth\": 10,\n",
    "        \"min_samples_split\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trainer = ModelTrainer(random_state=42)\n",
    "trained_models = trainer.train_multiple_models(X_train, y_train, model_configs)\n",
    "\n",
    "print(\"‚úÖ All models trained successfully!\")"
   ],
   "id": "7859aaa16270cae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-Validation Analysis",
   "id": "e3526bc9415c319f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform cross-validation for each model\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Performing cross-validation...\")\n",
    "for name, model in trained_models.items():\n",
    "    cv_result = cross_validate_model(\n",
    "        model, X_train, y_train, cv=5, scoring='accuracy'\n",
    "    )\n",
    "    cv_results[name] = cv_result\n",
    "    print(f\"{name:20s} - CV Score: {cv_result['mean_score']:.4f} (¬±{cv_result['std_score']:.4f})\")"
   ],
   "id": "8e3e62443f4f1b43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation and Comparison",
   "id": "3135d496643fe360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare all models on test set\n",
    "comparison_df = compare_models(trained_models, X_test, y_test, average='weighted')\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(4))"
   ],
   "id": "50d271c2c2c6151f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detailed Performance Visualization",
   "id": "f06d463e72d759a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create visualizations for model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "axes[0, 0].bar(comparison_df.index, comparison_df['accuracy'])\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. F1-Score comparison\n",
    "axes[0, 1].bar(comparison_df.index, comparison_df['f1_score'])\n",
    "axes[0, 1].set_title('Model F1-Score Comparison')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Precision vs Recall\n",
    "axes[1, 0].scatter(comparison_df['recall'], comparison_df['precision'], s=100)\n",
    "for i, model in enumerate(comparison_df.index):\n",
    "    axes[1, 0].annotate(model,\n",
    "                       (comparison_df['recall'].iloc[i], comparison_df['precision'].iloc[i]),\n",
    "                       xytext=(5, 5), textcoords='offset points')\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('Precision vs Recall')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Multiple metrics radar chart\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "metrics_df = comparison_df[metrics_to_plot]\n",
    "\n",
    "ax = axes[1, 1]\n",
    "angles = np.linspace(0, 2*np.pi, len(metrics_to_plot), endpoint=False)\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "for idx, model in enumerate(metrics_df.index[:3]):  # Plot top 3 models\n",
    "    values = metrics_df.loc[model].values\n",
    "    values = np.concatenate((values, [values[0]]))\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_to_plot)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Radar Chart')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5af0c3736672b650",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best Model Analysis",
   "id": "abe982096eb00165"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the best performing model\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {comparison_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {comparison_df.loc[best_model_name, 'f1_score']:.4f}\")\n",
    "\n",
    "# Detailed evaluation of best model\n",
    "evaluator = ClassificationEvaluator(model_name=best_model_name)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Print comprehensive evaluation\n",
    "print_evaluation_summary(\n",
    "    y_test, y_pred, y_pred_proba,\n",
    "    model_name=best_model_name,\n",
    "    class_names=['Malignant', 'Benign']\n",
    ")"
   ],
   "id": "55b894c8789a9ab1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualization: Confusion Metrices",
   "id": "31dac763e723eb91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot confusion matrices for top 3 models\n",
    "top_3_models = comparison_df.head(3).index\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, model_name in enumerate(top_3_models):\n",
    "    model = trained_models[model_name]\n",
    "    evaluator = ClassificationEvaluator(model_name=model_name)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    ax = axes[idx]\n",
    "    cm = evaluator.compute_confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', square=True, ax=ax)\n",
    "    ax.set_title(f'{model_name}\\nAccuracy: {comparison_df.loc[model_name, \"accuracy\"]:.3f}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "704238b9c99bd527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ROC Curves Comparison",
   "id": "81b0e18dd105a055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot ROC curves for all models (binary classification)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get positive class probabilities\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "40b786f656b159b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Importance Analysis (for tree-based models)",
   "id": "17f7ad5bdb7c3937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyze feature importance for tree-based models\n",
    "tree_based_models = ['Random Forest', 'Gradient Boosting', 'Decision Tree']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(tree_based_models), figsize=(18, 6))\n",
    "\n",
    "for idx, model_name in enumerate(tree_based_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_names = [f'Feature_{i}' for i in range(len(model.feature_importances_))]\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=True).tail(10)\n",
    "\n",
    "            axes[idx].barh(importance_df['feature'], importance_df['importance'])\n",
    "            axes[idx].set_title(f'{model_name}\\nTop 10 Features')\n",
    "            axes[idx].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f368c693dfb04848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Performance Summary",
   "id": "79bc37ecec636b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create final summary table\n",
    "summary_data = []\n",
    "\n",
    "for model_name in comparison_df.index:\n",
    "    cv_result = cv_results[model_name]\n",
    "    test_result = comparison_df.loc[model_name]\n",
    "\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV_Accuracy_Mean': cv_result['mean_score'],\n",
    "        'CV_Accuracy_Std': cv_result['std_score'],\n",
    "        'Test_Accuracy': test_result['accuracy'],\n",
    "        'Test_F1_Score': test_result['f1_score'],\n",
    "        'Test_Precision': test_result['precision'],\n",
    "        'Test_Recall': test_result['recall']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.round(4)\n",
    "\n",
    "print(\"üìä Final Model Performance Summary:\")\n",
    "print(\"=\" * 100)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Highlight the best model\n",
    "best_model_idx = summary_df['Test_F1_Score'].idxmax()\n",
    "print(f\"\\nüèÜ Recommended Model: {summary_df.loc[best_model_idx, 'Model']}\")\n",
    "print(f\"   - Cross-validation accuracy: {summary_df.loc[best_model_idx, 'CV_Accuracy_Mean']:.4f} (¬±{summary_df.loc[best_model_idx, 'CV_Accuracy_Std']:.4f})\")\n",
    "print(f\"   - Test accuracy: {summary_df.loc[best_model_idx, 'Test_Accuracy']:.4f}\")\n",
    "print(f\"   - Test F1-score: {summary_df.loc[best_model_idx, 'Test_F1_Score']:.4f}\")"
   ],
   "id": "c07271226b7117c9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
